{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load NextItemPredTransformer\n",
    "\n",
    "from NextItemPredTransformer import NextItemPredTransformer\n",
    "from NextItemPredTransformer import ModelDimensions\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size includes the SOS and EOS tokens\n",
    "vocab_size = 100\n",
    "# Initialize a random item embedding matrix\n",
    "item_embedding_matrix = torch.rand(vocab_size, 64)\n",
    "# Initialize a random user embedding matrix\n",
    "user_embedding_matrix = torch.rand(vocab_size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init NextItemPredTransformer\n",
    "dims = ModelDimensions(\n",
    "    model_input_length=102,\n",
    "    model_hidden_dim=64,\n",
    "    n_attention_heads=8,\n",
    "    n_decoder_layers=3,\n",
    "    vocab_size=vocab_size,\n",
    "    pre_trained_item_embeddings=item_embedding_matrix,\n",
    "    pre_trained_user_embeddings=user_embedding_matrix,\n",
    "    use_concat_user_embedding=True,\n",
    ")\n",
    "\n",
    "model = NextItemPredTransformer(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyper parameters\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "n_epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list from 0 to 10\n",
    "user_ids = list(range(10))\n",
    "# list of 10 random arrays of shape (10) where random numbers are between 0 and 100\n",
    "user_items_vectors = [torch.randint(0, 100, (10,)) for _ in range(10)]\n",
    "# list of 10 random arrays of shape (10)\n",
    "user_rating_times_vectors = [torch.rand(10) for _ in range(10)]\n",
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\utils.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  items[i, 1 : len(user_items) + 1] = torch.tensor(user_items)\n",
      "c:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\utils.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  times[i, 1 : len(user_rating_times_vectors[i]) + 1] = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "from NextItemPredDataset import NextItemPredDataset\n",
    "# Create a dataset\n",
    "dataset = NextItemPredDataset(\n",
    "    user_ids, user_items_vectors, user_rating_times_vectors, max_seq_len\n",
    ")\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[39m=\u001b[39m model(items, user_ids, times)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Create empty tensor of shape (batch_size, 1, dim)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# for i, pred in enumerate(pred_index):\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#     # For each batch take the relevant index from the output based on the pred_index for that batch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m#     relevant_outputs[i] = outputs[i][pred]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Convert this loop to a tensor operation\u001b[39;00m\n\u001b[0;32m     15\u001b[0m relevant_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(outputs, \u001b[39m1\u001b[39m, pred_index\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size))\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\NextItemPredTransformer.py:237\u001b[0m, in \u001b[0;36mNextItemPredTransformer.forward\u001b[1;34m(self, items, user, user_interactions_times)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m     items: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m    234\u001b[0m     user: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    235\u001b[0m     user_interactions_times: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 237\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(items, user, user_interactions_times\u001b[39m=\u001b[39;49muser_interactions_times)\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\NextItemPredTransformer.py:185\u001b[0m, in \u001b[0;36mItemDecoder.forward\u001b[1;34m(self, items, user, kv_cache, user_interactions_times)\u001b[0m\n\u001b[0;32m    179\u001b[0m items \u001b[39m=\u001b[39m (\n\u001b[0;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems_embedding(items)\n\u001b[0;32m    181\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding[offset : offset \u001b[39m+\u001b[39m items\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[0;32m    182\u001b[0m )\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m user_interactions_times \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     items \u001b[39m=\u001b[39m items \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_embedding(user_interactions_times)\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers_embedding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m user \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     embed_user \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers_embedding(user)\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "# Create a training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        user_ids, items, times, pred_index, true_item_id = batch\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(items, user_ids, times)\n",
    "        # Create empty tensor of shape (batch_size, 1, dim)\n",
    "        # for i, pred in enumerate(pred_index):\n",
    "        #     # For each batch take the relevant index from the output based on the pred_index for that batch\n",
    "        #     relevant_outputs[i] = outputs[i][pred]\n",
    "        # Convert this loop to a tensor operation\n",
    "        relevant_outputs = torch.gather(outputs, 1, pred_index.unsqueeze(1).unsqueeze(2).expand(-1, -1, vocab_size))\n",
    "        \n",
    "        # Squeeze dim 1 for relevant_outputs\n",
    "        relevant_outputs = torch.squeeze(relevant_outputs, dim=1)\n",
    "        loss = criterion(relevant_outputs, true_item_id)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4949604dacae39bdd2618bc76fb269ae5c6ee85cf584bc2addf7917737d48aad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
