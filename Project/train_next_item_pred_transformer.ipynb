{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load NextItemPredTransformer\n",
    "\n",
    "from NextItemPredTransformer import NextItemPredTransformer\n",
    "from NextItemPredTransformer import ModelDimensions\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size includes the SOS and EOS tokens\n",
    "vocab_size = 100\n",
    "# Initialize a random item embedding matrix\n",
    "item_embedding_matrix = torch.rand(vocab_size, 64)\n",
    "# Initialize a random user embedding matrix\n",
    "user_embedding_matrix = torch.rand(vocab_size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init NextItemPredTransformer\n",
    "dims = ModelDimensions(\n",
    "    model_input_length=102,\n",
    "    model_hidden_dim=40,\n",
    "    n_attention_heads=8,\n",
    "    n_decoder_layers=3,\n",
    "    vocab_size=vocab_size,\n",
    "    pre_trained_item_embeddings=item_embedding_matrix,\n",
    "    pre_trained_user_embeddings=user_embedding_matrix,\n",
    "    use_concat_user_embedding=True,\n",
    ")\n",
    "\n",
    "model = NextItemPredTransformer(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyper parameters\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "n_epochs = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list from 0 to 10\n",
    "user_ids = list(range(10))\n",
    "# list of 10 random arrays of shape (10) where random numbers are between 0 and 100\n",
    "user_items_vectors = [torch.randint(0, 100, (10,)) for _ in range(10)]\n",
    "# list of 10 random arrays of shape (10)\n",
    "user_rating_times_vectors = [torch.rand(10) for _ in range(10)]\n",
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\utils.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  items[i, 1 : len(user_items) + 1] = torch.tensor(user_items)\n",
      "c:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\utils.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  times[i, 1 : len(user_rating_times_vectors[i]) + 1] = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "from NextItemPredDataset import NextItemPredDataset\n",
    "# Create a dataset\n",
    "dataset = NextItemPredDataset(\n",
    "    user_ids, user_items_vectors, user_rating_times_vectors, max_seq_len\n",
    ")\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m model(items, user_ids, times)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Create empty tensor of shape (batch_size, 1, dim)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# for i, pred in enumerate(pred_index):\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#     # For each batch take the relevant index from the output based on the pred_index for that batch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m#     relevant_outputs[i] = outputs[i][pred]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Convert this loop to a tensor operation\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m relevant_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(outputs, \u001b[39m1\u001b[39m, pred_index\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size))\n\u001b[0;32m     17\u001b[0m \u001b[39m# Squeeze dim 1 for relevant_outputs\u001b[39;00m\n\u001b[0;32m     18\u001b[0m relevant_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(relevant_outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m model(items, user_ids, times)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Create empty tensor of shape (batch_size, 1, dim)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# for i, pred in enumerate(pred_index):\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#     # For each batch take the relevant index from the output based on the pred_index for that batch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m#     relevant_outputs[i] = outputs[i][pred]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Convert this loop to a tensor operation\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m relevant_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(outputs, \u001b[39m1\u001b[39m, pred_index\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size))\n\u001b[0;32m     17\u001b[0m \u001b[39m# Squeeze dim 1 for relevant_outputs\u001b[39;00m\n\u001b[0;32m     18\u001b[0m relevant_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(relevant_outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\Recommendation Systems\\Project\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        user_ids, items, times, pred_index, true_item_id = batch\n",
    "        # These are the actual times that we want to predict the next item for(simulates user recommendation time)\n",
    "        pred_times = times[:, pred_index]\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(items, user_ids, times, pred_times)\n",
    "        # Create empty tensor of shape (batch_size, 1, dim)\n",
    "        # for i, pred in enumerate(pred_index):\n",
    "        #     # For each batch take the relevant index from the output based on the pred_index for that batch\n",
    "        #     relevant_outputs[i] = outputs[i][pred]\n",
    "        # Convert this loop to a tensor operation\n",
    "        relevant_outputs = torch.gather(outputs, 1, pred_index.unsqueeze(1).unsqueeze(2).expand(-1, -1, vocab_size))\n",
    "        \n",
    "        # Squeeze dim 1 for relevant_outputs\n",
    "        relevant_outputs = torch.squeeze(relevant_outputs, dim=1)\n",
    "        loss = criterion(relevant_outputs, true_item_id)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4949604dacae39bdd2618bc76fb269ae5c6ee85cf584bc2addf7917737d48aad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
