{"cells":[{"cell_type":"markdown","metadata":{"id":"Jj3d0tnGwRsL"},"source":["# Recommendations Systems\n","## Assignment 3:  Neural Collaborative Filtering"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XSKQQt41wRsN"},"source":["**By:**  \n","Group 16\n","\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"ykxbPEPKwRsO"},"source":["**The goal of this assignment is to:**\n","- Understand the concept of recommendations based on implicit data which is very common in real life.\n","- Understand how DL components can be used to implement a collaborative filtering & hybrid approach recommenders.\n","- Understand pros&cons comparing to other recommender system approaches.\n","- Practice recommender system training and evaluation.\n","\n","**Instructions:**\n","- Students will form teams of two people each, and submit a single homework for each team.\n","- The same score for the homework will be given to each member of the team.\n","- Your solution in the form of an Jupyter notebook file (with extension ipynb).\n","- Images/Graphs/Tables should be submitted inside the notebook.\n","- The notebook should be runnable and properly documented. \n","- Please answer all the questions and include all your code.\n","- English only.\n","\n","**Submission:**\n","- Submission of the homework will be done via Moodle by uploading a link to google colab.\n","- The homwork needs to be entirely in English.\n","- The deadline for submission is on Moodle.\n","\n","**Requirements:**  \n","- Python 3.6+ should be used. \n","- You may use Torch/Keras/TF packeges.\n","- You should implement the recommender system by yourself using only basic Python libraries (such as numpy)."]},{"cell_type":"markdown","metadata":{"id":"XP3eoAsewRsO"},"source":["**LINKS:**\n","- <a href='https://github.com/hexiangnan/neural_collaborative_filtering/tree/master/Data'>Dataset</a>\n","- <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>Repository</a>\n","- <a href='https://towardsdatascience.com/paper-review-neural-collaborative-filtering-explanation-implementation-ea3e031b7f96'>Blog Post Review</a>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"SfHgAi5SwRsO"},"source":["**Google <a href='https://colab.research.google.com/'>Colaboratory</a>**  \n","        \n","    This is a great academic tool for students. Instead of installing and running \"everything\" on your Laptop - which probably will take you a lot of time - you can use Google Colab.  \n","    Basically, you can use it for all your Python needs.  \n","\n","**PyTorch <a href='https://pytorch.org/tutorials/beginner/basics/intro.html'>Tutorials</a>**   \n","    \n","    Just follow steps 0-7 and you will have the basics skills to understand, build, and run DL recommender models. \n","\n","**Keras Kaggle's <a href='https://www.kaggle.com/learn/intro-to-deep-learning'>intro-to-deep-learning</a>**  \n","    \n","    This will give you a quick idea of what DL is, and how to utilize it.  \n","    They're using TensorFlow, while in our MLDL program we're using PyTorch.  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5CwuJFgRwRsP"},"source":["**Grading:**\n","\n","- Q1 - 20 points - Dataset Preparation\n","- Q2 - 50 points - Neural Collaborative Filtering\n","- Q3 - 30 points - Loss Function\n","\n","`Total: 100`\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"vjq5fTiwwRsP"},"source":["**Prerequisites**"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"ys_8dHHrwRsP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 21.2.3; however, version 22.3.1 is available.\n","You should consider upgrading via the 'c:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"]}],"source":["%pip install torch torchvision --quiet"]},{"cell_type":"markdown","metadata":{"id":"Awiq5GVNwRsQ"},"source":["**Imports**"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"8zbwlB8DwRsQ"},"outputs":[],"source":["# basic\n","import os \n","import sys\n","import math\n","import heapq\n","import argparse\n","from time import time\n","import multiprocessing\n","\n","# general\n","import warnings\n","import numpy as np\n","import scipy as sp\n","import pandas as pd\n","import scipy.sparse as sp\n","\n","# visual\n","import matplotlib\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# visual 3D\n","from mpl_toolkits import mplot3d\n","\n","# notebook\n","from IPython.display import display, HTML\n","\n","\n","# torch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential\n","from torch.nn import Sigmoid,ReLU\n","from torch.nn import Embedding,Linear,Dropout\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.transforms import ToTensor,Compose\n","from torch.optim import SparseAdam,Adam,Adagrad,SGD\n","\n","# colab\n","# from google.colab import drive  "]},{"cell_type":"markdown","metadata":{"id":"yNInRwuOwRsQ"},"source":["**Hide Warnings**"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"lGl6YxtjwRsR"},"outputs":[],"source":["warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"lqWShK-EwRsR"},"source":["**Disable Autoscrolling**"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"U63LxRbUwRsR","outputId":"c6cffba1-412a-4842-8a6f-e9a8f39f47e5"},"outputs":[{"data":{"application/javascript":"IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n};\n","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["%%javascript\n","IPython.OutputArea.prototype._should_scroll = function(lines) {\n","    return false;\n","};"]},{"cell_type":"markdown","metadata":{"id":"AJxZJ6ijwRsR"},"source":["<br><br><br>\n","<br><br><br>\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"l6lCdQbzwRsR"},"source":["## Question 1: Dataset Preparation (Ingestion)\n","\n","---\n","\n","\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"9pOmzAspwRsR"},"source":["This implementation contains one file for training and two files for testing:   \n","- ml-1m.train.rating   \n","- ml-1m.test.rating  \n","- ml-1m.test.negative   \n","\n","<br>\n","(feel free to use visual explanations)\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"4WNea9kswRsR"},"source":["a. **Explain** the role and structure of each file and how it was created from the original <a href='https://github.com/hexiangnan/neural_collaborative_filtering/tree/master/Data'>MovieLens 1M rating dataset</a>."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- train rating file consists of positive instances. Each line in the file is of the following format: user_id item_id rating time_stamp .\n","- test rating file consists of positive instances (basically last sample for each user of interaction which isn't in train). Each line in the file is of the following format: user_id item_id rating time_stamp . One line per user.\n","- Negative sample file is file which each line corresponds to user id and test item(positive) and 99 items ids which were negatively sampled, format: (user_id,item_id) , negative sample1,negative sample2, ..."]},{"cell_type":"markdown","metadata":{"id":"9C6h5NW8wRsS"},"source":["b. **Explain** how the training dataset is created."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- We iterate through each line of the train file.\n","- We find the number of items and users.\n","- We create a sparse matrix with NXM (users X items)\n","- We go through the file again and for each user item pair in the file line we check if there has been rating with a value greater than 0.\n","- If there has been a positive rating we add a binary indicator of implicit feedback (1)."]},{"cell_type":"markdown","metadata":{"id":"AknICV35wRsS"},"source":["c. **Explain** how the test dataset is created."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- test ratings list:\n"," is a list with tuples of user, a positively rated item(which isn't in the train set).\n","- negative sampling list:\n","Each line in negative file contains 99 negative samples(the id of the user matches the index of the line of test ratings file).\n","\n","Then during the evaluation we will evaluate the 99 negative samples and the positive sample from the test list."]},{"cell_type":"markdown","metadata":{"id":"BYdz5IwnwRsS"},"source":["#### Data Preperations:"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"HEvP5dKkwRsS"},"outputs":[],"source":["# urls\n","train_url = \"https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/Data/ml-1m.train.rating?raw=true\"\n","test_url = \"https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/Data/ml-1m.test.rating?raw=true\"\n","test_neg_url = \"https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/Data/ml-1m.test.negative?raw=true\"\n"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["import requests\n","\n","def get_num_users_and_items(data_lines):\n","    num_users = 0\n","    num_items = 0\n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        user_id = int(line_values[0])\n","        item_id = int(line_values[1])\n","        num_users = max(num_users, user_id)\n","        num_items = max(num_items, item_id)\n","\n","    return num_users, num_items\n","\n","def generate_matrix(data_lines, num_users, num_items):\n","    matrix = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        user_id = int(line_values[0])\n","        item_id = int(line_values[1])\n","        rating = float(line_values[2])\n","        if rating > 0:\n","            matrix[user_id, item_id] = 1.0\n","    \n","    return matrix\n","    \n","\n","def load_data_as_matrix(url):\n","    # download data with requests\n","    response = requests.get(url)\n","    # Read as a text file\n","    data = response.text\n","    # Split by lines\n","    data_lines = data.splitlines()\n","    # Get number of users and items\n","    num_users, num_items = get_num_users_and_items(data_lines)\\\n","    # Construct matrix\n","    mat = generate_matrix(data_lines, num_users, num_items)\n","    \n","    return mat\n","\n","def load_negatives_vector(url):\n","    # download data with requests\n","    response = requests.get(url)\n","    # Read as a text file\n","    data = response.text\n","    # Split by lines\n","    data_lines = data.splitlines()\n","    # Construct vector\n","    vector = []\n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        vector.append([int(x) for x in line_values[1:]])\n","    \n","    return vector\n","\n","def load_data_as_list(url):\n","    res = []\n","    # download data with requests\n","    response = requests.get(url)\n","    # Read as a text file\n","    data = response.text\n","    # Split by lines\n","    data_lines = data.splitlines()\n","    \n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        user_id = int(line_values[0])\n","        item_id = int(line_values[1])\n","        res.append([user_id, item_id])\n","\n","    return res\n","     \n"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"VpxacomIwRsS"},"outputs":[],"source":["train_matrix = load_data_as_matrix(train_url)\n","test_ratings = load_data_as_list(test_url)\n","test_neg_vector = load_negatives_vector(test_neg_url)\n","\n","num_users, num_items = train_matrix.shape"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"_76sxTaFwRsS","outputId":"35463ebf-4362-43b9-fda3-082295f80f06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train matrix shape:  (6040, 3706)\n","Test matrix shape:  6040\n","Test negative vector shape:  6040\n","99\n"]}],"source":["# print shapes of train and test matrices\n","print(\"Train matrix shape: \", train_matrix.shape)\n","print(\"Test matrix shape: \", len(test_ratings))\n","\n","# print shape of test negative vector\n","print(\"Test negative vector shape: \", len(test_neg_vector))\n","print(len(test_neg_vector[0]))"]},{"cell_type":"markdown","metadata":{"id":"5OgOFP1BwRsS"},"source":["<br><br><br>\n","<br><br><br>\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"BZohC5TfwRsS"},"source":["## Question 2: Neural Collaborative Filtering \n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"9Ux4hWQPwRsS"},"source":["## a. Build the following four models using the neural collaborative filtering approach: \n","- Matrix Factorization (MF)\n","- Multi layer perceptron (MLP)\n","- Generalized Matrix Factorization (GMF) \n","- NeuroMatrixFactorization (NMF)\n","\n","**For each model, use the best hyper-parameters suggested in the neuMF paper.**"]},{"cell_type":"markdown","metadata":{"id":"5Duot--dwRsS"},"source":["<br><br><br><br>\n","#### Matrix Factorization (MF)  \n","<br>"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["class MF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_size=32):\n","        super(MF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.emb_size = embedding_size\n","        self.emb_item = nn.Embedding(num_embeddings=num_items, embedding_dim=self.emb_size)\n","        self.emb_user = nn.Embedding(num_embeddings=num_users, embedding_dim=self.emb_size)\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        nn.init.normal_(self.emb_user.weight, std=0.01)\n","        nn.init.normal_(self.emb_item.weight, std=0.01)\n","\n","    def forward(self, users, items):\n","        emb_user = self.emb_user(users)\n","        emb_item = self.emb_item(items)\n","        return emb_user @ emb_item.T"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["model_MF = MF(num_users, num_items, embedding_size=32)"]},{"cell_type":"markdown","metadata":{"id":"vitvozFFEBzq"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"wgch4APIECF4"},"outputs":[{"name":"stdout","output_type":"stream","text":["MF(\n","  (emb_item): Embedding(3706, 32)\n","  (emb_user): Embedding(6040, 32)\n",")\n"]}],"source":["# display/print the model architecture\n","print(model_MF)"]},{"cell_type":"markdown","metadata":{"id":"Dh-zg0azwRsS"},"source":["<br><br><br><br><br><br>\n","#### Multi Layer Perceptron (MLP)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_size=16, mlp_layers_sizes=[32, 16, 8], dropout=0.1):\n","        super(MLP, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.embedding_size = embedding_size\n","        self.mlp_layers_sizes = mlp_layers_sizes\n","        self.dropout = dropout\n","\n","        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_size)\n","        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=embedding_size)\n","        self.mlp_layers = nn.Sequential(\n","            nn.Linear(2 * embedding_size, mlp_layers_sizes[0]),\n","            nn.ReLU(),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(mlp_layers_sizes[0], mlp_layers_sizes[1]),\n","            nn.ReLU(),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(mlp_layers_sizes[1], mlp_layers_sizes[2]),\n","        )\n","        self.last_hidden = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(mlp_layers_sizes[2], 1)\n","        )\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, user_ids, item_ids):\n","        user_embedding = self.user_embedding(user_ids)\n","        item_embedding = self.item_embedding(item_ids)\n","        input_vector = torch.cat([user_embedding, item_embedding], dim=1)\n","        output = self.mlp_layers(input_vector)\n","        output = self.activation(output)\n","        return output\n","\n","        "]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["model_MLP = MLP(num_users, num_items, embedding_size=16, mlp_layers_sizes=[32, 16, 8], dropout=0.1)"]},{"cell_type":"markdown","metadata":{"id":"JQQhkHQ8EWkM"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"1rucXJW0EWpa"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP(\n","  (user_embedding): Embedding(6040, 16)\n","  (item_embedding): Embedding(3706, 16)\n","  (mlp_layers): Sequential(\n","    (0): Linear(in_features=32, out_features=32, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.1, inplace=False)\n","    (3): Linear(in_features=32, out_features=16, bias=True)\n","    (4): ReLU()\n","    (5): Dropout(p=0.1, inplace=False)\n","    (6): Linear(in_features=16, out_features=8, bias=True)\n","  )\n","  (last_hidden): Sequential(\n","    (0): ReLU()\n","    (1): Dropout(p=0.1, inplace=False)\n","    (2): Linear(in_features=8, out_features=1, bias=True)\n","  )\n","  (activation): Sigmoid()\n",")\n"]}],"source":["# display/print the model architecture\n","print(model_MLP)"]},{"cell_type":"markdown","metadata":{"id":"D_CI9v2ewRsS"},"source":["<br><br><br><br><br><br>\n","####Generalized Matrix Factorization (GMF)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["class GMF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_size=32):\n","        super(GMF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.emb_size = embedding_size\n","        self.emb_item = nn.Embedding(num_embeddings=num_items, embedding_dim=self.emb_size)\n","        self.emb_user = nn.Embedding(num_embeddings=num_users, embedding_dim=self.emb_size)\n","        self.hidden = torch.nn.Linear(self.emb_size, 1) \n","        self.activation = nn.Sigmoid()\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        nn.init.normal_(self.emb_user.weight, std=0.01)\n","        nn.init.normal_(self.emb_item.weight, std=0.01)\n","\n","    def forward(self, users, items):\n","        emb_user = self.emb_user(users)\n","        emb_item = self.emb_item(items)\n","        element_wise = emb_user * emb_item\n","        output = self.hidden(element_wise)\n","        output = self.activation(output)\n","        \n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["model_GMF = GMF(num_users, num_items,embedding_size = 32)"]},{"cell_type":"markdown","metadata":{"id":"4eSt1BbiEZqa"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"clyfpE6BEZxR"},"outputs":[{"name":"stdout","output_type":"stream","text":["GMF(\n","  (emb_item): Embedding(3706, 32)\n","  (emb_user): Embedding(6040, 32)\n","  (hidden): Linear(in_features=32, out_features=1, bias=True)\n","  (activation): Sigmoid()\n",")\n"]}],"source":["# display/print the model architecture\n","print(model_GMF)"]},{"cell_type":"markdown","metadata":{"id":"M6BRGoaIwRsT"},"source":["<br><br><br><br><br><br>\n","#### NeuroMatrixFactorization (NMF)\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["# Note for the simplicity of the implementation I've decided not to take mlp and gmf as backbones(like it was done in the paper)\n","class NCF(nn.Module):\n","    def __init__(\n","        self,\n","        num_users,\n","        num_items,\n","        embedding_size=32,\n","        mlp_embedding_size=16,\n","        mlp_layers_sizes=[32, 16, 8],\n","        dropout=0.1,\n","    ):\n","        super(NCF, self).__init__()\n","        self.mlp = MLP(\n","            num_users,\n","            num_items,\n","            embedding_size=mlp_embedding_size,\n","            mlp_layers_sizes=mlp_layers_sizes,\n","            dropout=dropout,\n","        )\n","        self.gmf = GMF(num_users, num_items, embedding_size=embedding_size)\n","        self.neu_mf = nn.Linear(mlp_layers_sizes[-1] + embedding_size, 1)\n","        self.activation = nn.Sigmoid()\n","        self.mute_last_layers()\n","\n","    def mute_last_layers(self):\n","        self.mlp.activation = nn.Identity()\n","        self.gmf.activation = nn.Identity()\n","        self.gmf.hidden = nn.Identity()\n","        self.mlp.last_hidden = nn.Identity()\n","\n","    def forward(self, users, items):\n","        mlp_output = self.mlp(users, items)\n","        gmf_output = self.gmf(users, items)\n","        output = torch.cat([mlp_output, gmf_output], dim=1)\n","        output = self.neu_mf(output)\n","        output = self.activation(output)\n","        return output\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["model_NMF = NCF(num_users, num_items)"]},{"cell_type":"markdown","metadata":{"id":"0O-ttN8PEeNC"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"bypupfsrEeSE"},"outputs":[{"name":"stdout","output_type":"stream","text":["NCF(\n","  (mlp): MLP(\n","    (user_embedding): Embedding(6040, 16)\n","    (item_embedding): Embedding(3706, 16)\n","    (mlp_layers): Sequential(\n","      (0): Linear(in_features=32, out_features=32, bias=True)\n","      (1): ReLU()\n","      (2): Dropout(p=0.1, inplace=False)\n","      (3): Linear(in_features=32, out_features=16, bias=True)\n","      (4): ReLU()\n","      (5): Dropout(p=0.1, inplace=False)\n","      (6): Linear(in_features=16, out_features=8, bias=True)\n","    )\n","    (last_hidden): Identity()\n","    (activation): Identity()\n","  )\n","  (gmf): GMF(\n","    (emb_item): Embedding(3706, 32)\n","    (emb_user): Embedding(6040, 32)\n","    (hidden): Identity()\n","    (activation): Identity()\n","  )\n","  (neu_mf): Linear(in_features=40, out_features=1, bias=True)\n","  (activation): Sigmoid()\n",")\n"]}],"source":["# display/print the model architecture\n","print(model_NMF)"]},{"cell_type":"markdown","metadata":{"id":"BxrAtkliwRsT"},"source":["<br><br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"xThyFGsHwRsT"},"source":["## b. Train and evaluate the recommendations accuracy of the models: \n","- MF\n","- GMF\n","- MLP\n","- NMF\n","\n","Compare the `LogLoss` and recommendations accuracy using `NDCG` and `MRR` metrics with cutoff values of 5 and 10.   \n","Discuss the comparison. "]},{"cell_type":"markdown","metadata":{"id":"eOKodzLSwRsT"},"source":["**Metrics:**\n","- HitRatio\n","- nDCG\n","- MRR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOHGH9acwRsT"},"outputs":[],"source":["# Use your own metrics implementation OR use external packages for the metrics.\n","# If you are using external packages make sure they work properly. \n","# A lot of the packages available does not work as you would expect."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import List\n","\n","def compute_hit_rate(pred: np.ndarray, pred_items_ids: np.ndarray, gt_items_ids_per_user: List[int], top_n=10) -> float:\n","        scores = [\n","            compute_hit_rate_for_user(pred[i], pred_items_ids[i], gt_items_ids_per_user[i])\n","            for i in range(len(gt_items_ids_per_user))\n","        ]\n","        return np.mean(scores)\n","\n","def compute_hit_rate_for_user(user_pred: np.ndarray,  user_pred_item_ids: np.ndarray, gt_item_id: int, top_n=10) -> int:\n","    top_n_indices = np.argsort(user_pred)[::-1][:top_n]\n","    # Get ids of the top n items\n","    top_n_item_ids = user_pred_item_ids[top_n_indices]\n","    # Check if the top n indices contains the index of the test item\n","    return 1 if gt_item_id in top_n_item_ids else 0\n","    "]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def compute_ndcg(\n","    pred: np.ndarray, pred_items_ids: np.ndarray, gt_items_ids_per_user: List[int], top_n=5\n",") -> float:\n","    \"\"\"Computes NDCG for all users\n","    Args:\n","        pred: predicted ratings for all users\n","        pred_items_ids: list of item ids of the predicted ratings for all users\n","        gt_items_ids_per_user: list of ground truth items ids for all users\n","        top_n: number of top items to consider\n","    Returns:\n","        Average NDCG score for all users\n","    \"\"\"\n","    scores = [\n","        compute_ndcg_for_user(pred[i], pred_items_ids[i], gt_items_ids_per_user[i], top_n)\n","        for i in range(len(gt_items_ids_per_user))\n","    ]\n","    return np.mean(scores)\n","\n","\n","def compute_ndcg_for_user(\n","    user_pred: np.ndarray, user_pred_item_ids: np.ndarray, gt_item_id: int, top_n=5\n",") -> float:\n","    \"\"\"Computes NDCG for a single user.\n","    Args:\n","        user_pred: list of predicted ratings for a single user\n","        user_pred_item_ids: list of item ids of the predicted ratings for a single user\n","        gt_item_id id of the ground truth item\n","        top_n: number of top items to consider\n","    Returns:\n","        NDCG for a single user\n","    \"\"\"\n","    # Get the indices of the top_n items in user pred ratings\n","    top_n_indices = np.argsort(user_pred)[::-1][:top_n]\n","    # Get the indices of the top_n items in user pred item ids\n","    top_n_item_ids = user_pred_item_ids[top_n_indices]\n","\n","    for i in range(top_n):\n","        if top_n_item_ids[i] == gt_item_id:\n","            # We know that idcg is 1 for this case we we can just return the dcg\n","            return 1 / np.log2(i + 2)\n","    return 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def compute_mrr_for_user(user_pred: np.ndarray, user_pred_item_ids: np.ndarray, gt_item_id: int, top_n=5) -> float:\n","        \"\"\" Computes MRR for a single user.\n","        Args:\n","            user_pred: list of predicted ratings for a single user\n","            gt_item_index: index of the ground truth item\n","            top_n: number of top items to consider\n","        Returns:\n","            MRR for a single user\n","        \"\"\"\n","        top_n_indices = user_pred[:top_n]\n","        # Get the indices of the top_n items in user pred item ids\n","        top_n_item_ids = user_pred_item_ids[top_n_indices]\n","\n","        for i in range(top_n):\n","            if top_n_item_ids[i] == gt_item_id:\n","                return 1 / (i + 1)\n","        # if there is no match, return 0\n","        return 0\n","\n","def compute_mrr(pred: np.ndarray, pred_items_ids: np.ndarray,  gt_items_ids_per_user: List[int], top_n=5) -> float:\n","        \"\"\" Mean Reciprocal Rank for all users \"\"\"\n","        scores = [\n","            compute_mrr_for_user(pred[i], pred_items_ids[i], gt_items_ids_per_user[i], top_n)\n","            for i in range(len(gt_items_ids_per_user))\n","        ]\n","        return np.mean(scores)"]},{"cell_type":"markdown","metadata":{"id":"PW2FyWqVwRsT"},"source":["**Evaluation:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KX6JtH5zwRsT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwPhLbr9wRsT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Ufqag0i5wRsT"},"source":["**HyperParams:**"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"Gf-Ku4H2wRsT"},"outputs":[],"source":["# the choosen hyperparams will effect your models & your grade \n","from torch.optim import lr_scheduler\n","\n","#TODO: create these params for each model\n","# optimizer = torch.optim.Adam(model_NMF.parameters(), lr=0.001)\n","# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n"]},{"cell_type":"markdown","metadata":{"id":"Bb3_Vll6wRsT"},"source":["<br><br>\n","Create train data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_rwb27KwRsT"},"outputs":[],"source":["# Create pytorch DATASET for the dataset\n","\n","class MovieLens(Dataset):\n","    def __init__(self, ratings, users, items):\n","        self.ratings = ratings\n","        self.users = users\n","        self.items = items\n","\n","    def __len__(self):\n","        return len(self.ratings)\n","\n","    def __getitem__(self, idx):\n","        return self.users[idx], self.items[idx], self.ratings[idx]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Select torch device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Selected device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"FvTC1ZP1wRsT"},"source":["<br><br>\n","train & eval:\n","- Create a training function \n","- Evaluate the models trained and save the results accordingly "]},{"cell_type":"code","execution_count":52,"metadata":{"id":"SwH51NuBwRsT"},"outputs":[],"source":["from tqdm import tqdm\n","\n","@torch.enable_grad()\n","def model_train(model, ratings, criterion, optimizer, scheduler, batch_size=64, num_epochs=10):\n","    sample_row, sample_col = ratings.nonzero()\n","    n_samples = len(sample_row)\n","    training_indices = np.arange(n_samples)\n","    # TODO: add other metrics arrays here for history\n","    train_losses_history = []\n","\n","    for epoch in tqdm(range(num_epochs)):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","        np.random.shuffle(training_indices)\n","        training_indices_batches = np.array_split(training_indices, n_samples // batch_size)\n","        # Convert to torch tensor\n","        training_indices_batches = [torch.tensor(batch) for batch in training_indices_batches]\n","\n","        model.train()  # Set model to training mode\n","\n","        running_loss = 0.0   # total loss of the network at each epoch\n","            \n","            # Iterate over data.\n","        for batch in tqdm(training_indices_batches):\n","            item_indices = sample_col[batch]\n","            user_indices = sample_row[batch]\n","            user_indices = user_indices.to(device)\n","            item_indices = item_indices.to(device)    \n","        \n","\n","            optimizer.zero_grad()\n","\n","            pred = model(user_indices, item_indices)\n","            loss = criterion(pred, ratings[user_indices, item_indices].float())\n","            loss.backward()\n","            optimizer.step()\n","                    \n","            running_loss += loss.item()\n","\n","            if scheduler is not None:\n","                scheduler.step()\n","            \n","        epoch_loss = running_loss / len(training_indices_batches)\n","        \n","        print('Loss: {:.4f}'.format(epoch_loss))\n","            \n","        # save the metrics\n","        train_losses_history.append(epoch_loss)\n","\n","    return model, train_losses_history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create eval loop\n","@torch.no_grad()\n","def model_train(model, ratings, batch_size=64):\n","    sample_row, sample_col = ratings.nonzero()\n","    n_samples = len(sample_row)\n","    eval_indices = np.arange(n_samples)\n","\n","    model.eval()  # Set model to training mode\n","\n","    np.random.shuffle(eval_indices)\n","    eval_indices_batches = np.array_split(eval_indices, n_samples // batch_size)\n","\n","    # Convert to torch tensor\n","    eval_indices_batches = [torch.tensor(batch) for batch in eval_indices_batches]\n","\n","    # Iterate over data.\n","    for batch in tqdm(eval_indices_batches):\n","        item_indices = sample_col[batch]\n","        user_indices = user_indices.to(device)\n","        item_indices = item_indices.to(device)\n","        pred = model(user_indices, item_indices)\n","        # TODO: calculate metrics for batch\n","    \n","    # TODO: Get overall results as an average of the metrics\n","    # TODO: Return metrics\n","    return \"\"\n","\n","    \n","\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"mtJaEmEkwRsT"},"source":["<br><br><br><br>\n","<br><br><br><br>\n","<br><br><br><br>\n","All Results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MmU9RQTwRsT","outputId":"628a9795-209a-4d81-a1ff-3ff6ffe47fde"},"outputs":[],"source":["# df_results\n","# each hyperparam will add a column to the dataframe\n","# this is an example for a df that would allow you to create plots easily"]},{"cell_type":"markdown","metadata":{"id":"lKiZrguTwRsT"},"source":["<br><br><br><br>\n","**Train & Validation Loss:**\n","\n","Make sure you did not overfit.  \n","In case you did, fix that by adding early-stopping, regularization, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZABlCT8wRsT"},"outputs":[],"source":["# plot\n","# make sure that you did not overfit!"]},{"cell_type":"markdown","metadata":{"id":"PlRPIAEwwRsU"},"source":["**Training Time:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMnMj7tkwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"GAMKjPbewRsU"},"source":["**Metric Evaluation:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRV3bz6_wRsU","outputId":"0e0a8f34-b8cf-48f9-90ae-7cd8f0476099"},"outputs":[],"source":["# plot suggestion/example - you may create your own plot (you should achieve higher results)"]},{"cell_type":"markdown","metadata":{"id":"UIZFWr8fwRsU"},"source":["<br><br><br><br>\n","<br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"5U2L74YEwRsU"},"source":["**c. How do the values of MRR and NDCG differ between your current model and the results you got in the previous exercises which implemented the explicit recommendation approach? What are the differences in preparing the dataset for evaluation?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OthggCt7wRsU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8cLPAs2BwRsU"},"source":["**d. How will you measure item similarity using the NeuMF model?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TksQLQ12wRsU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rrx0Eq3owRsU"},"source":["<br><br><br>\n","<br><br><br>\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"hYqvTGpawRsU"},"source":["## Question 3: Loss Function \n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"PMvPHonlwRsU"},"source":["#### a. One of the enhancements presented in the Neural Collaborative Filtering paper is the usage of probabilistic activation function (the sigmoid) and binary cross entropy loss function.    \n","\n","Select one of the models you implemented in question 2 and change the loss function to a `Mean Squared Error` and the activation function of the last layer to `RELU`.   \n","\n","Train the model and evaluate it in a similar way to what you did in question 2. \n","Compare the results and discuss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwoyVWp6wRsU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1YEAjeBdwRsU"},"source":["<br><br><br><br>\n","<br><br><br><br>\n","NMFs Results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-W6ZWJrzwRsU"},"outputs":[],"source":["# example: df_results[df_results.model.str.startswith('NMF')]"]},{"cell_type":"markdown","metadata":{"id":"Y-4txgJ0wRsU"},"source":["<br><br><br>\n","<br><br><br>\n","\n","Train & Validation Loss:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sg45Gf3jwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"RMGjniP4wRsU"},"source":["<br><br><br>\n","Training Time:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIvazqFQwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"7w9hEWRhwRsU"},"source":["<br><br><br>\n","Metric Evaluation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2glYFiQwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"FpDDwEKBwRsU"},"source":["<br><br><br><br>\n","**Conclusions:**\n","\n","    - In\n","    - Your\n","    - Own\n","    - Words\n","    "]},{"cell_type":"markdown","metadata":{"id":"hLwmMt95wRsV"},"source":["<br><br>\n","<br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"NE0-2ifMwRsV"},"source":["Good Luck :)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"}}},"nbformat":4,"nbformat_minor":0}
