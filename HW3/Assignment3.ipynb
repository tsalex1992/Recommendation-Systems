{"cells":[{"cell_type":"markdown","metadata":{"id":"Jj3d0tnGwRsL"},"source":["# Recommendations Systems\n","## Assignment 3:  Neural Collaborative Filtering"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XSKQQt41wRsN"},"source":["**By:**  \n","Group 16\n","\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"ykxbPEPKwRsO"},"source":["**The goal of this assignment is to:**\n","- Understand the concept of recommendations based on implicit data which is very common in real life.\n","- Understand how DL components can be used to implement a collaborative filtering & hybrid approach recommenders.\n","- Understand pros&cons comparing to other recommender system approaches.\n","- Practice recommender system training and evaluation.\n","\n","**Instructions:**\n","- Students will form teams of two people each, and submit a single homework for each team.\n","- The same score for the homework will be given to each member of the team.\n","- Your solution in the form of an Jupyter notebook file (with extension ipynb).\n","- Images/Graphs/Tables should be submitted inside the notebook.\n","- The notebook should be runnable and properly documented. \n","- Please answer all the questions and include all your code.\n","- English only.\n","\n","**Submission:**\n","- Submission of the homework will be done via Moodle by uploading a link to google colab.\n","- The homwork needs to be entirely in English.\n","- The deadline for submission is on Moodle.\n","\n","**Requirements:**  \n","- Python 3.6+ should be used. \n","- You may use Torch/Keras/TF packeges.\n","- You should implement the recommender system by yourself using only basic Python libraries (such as numpy)."]},{"cell_type":"markdown","metadata":{"id":"XP3eoAsewRsO"},"source":["**LINKS:**\n","- <a href='https://github.com/hexiangnan/neural_collaborative_filtering/tree/master/Data'>Dataset</a>\n","- <a href='https://github.com/hexiangnan/neural_collaborative_filtering'>Repository</a>\n","- <a href='https://towardsdatascience.com/paper-review-neural-collaborative-filtering-explanation-implementation-ea3e031b7f96'>Blog Post Review</a>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"SfHgAi5SwRsO"},"source":["**Google <a href='https://colab.research.google.com/'>Colaboratory</a>**  \n","        \n","    This is a great academic tool for students. Instead of installing and running \"everything\" on your Laptop - which probably will take you a lot of time - you can use Google Colab.  \n","    Basically, you can use it for all your Python needs.  \n","\n","**PyTorch <a href='https://pytorch.org/tutorials/beginner/basics/intro.html'>Tutorials</a>**   \n","    \n","    Just follow steps 0-7 and you will have the basics skills to understand, build, and run DL recommender models. \n","\n","**Keras Kaggle's <a href='https://www.kaggle.com/learn/intro-to-deep-learning'>intro-to-deep-learning</a>**  \n","    \n","    This will give you a quick idea of what DL is, and how to utilize it.  \n","    They're using TensorFlow, while in our MLDL program we're using PyTorch.  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5CwuJFgRwRsP"},"source":["**Grading:**\n","\n","- Q1 - 20 points - Dataset Preparation\n","- Q2 - 50 points - Neural Collaborative Filtering\n","- Q3 - 30 points - Loss Function\n","\n","`Total: 100`\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"vjq5fTiwwRsP"},"source":["**Prerequisites**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ys_8dHHrwRsP"},"outputs":[],"source":["%pip install torch torchvision --quiet"]},{"cell_type":"markdown","metadata":{"id":"Awiq5GVNwRsQ"},"source":["**Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zbwlB8DwRsQ"},"outputs":[],"source":["# basic\n","import os \n","import sys\n","import math\n","import heapq\n","import argparse\n","from time import time\n","import multiprocessing\n","\n","# general\n","import warnings\n","import numpy as np\n","import scipy as sp\n","import pandas as pd\n","import scipy.sparse as sp\n","\n","# visual\n","import matplotlib\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# visual 3D\n","from mpl_toolkits import mplot3d\n","\n","# notebook\n","from IPython.display import display, HTML\n","\n","\n","# torch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential\n","from torch.nn import Sigmoid,ReLU\n","from torch.nn import Embedding,Linear,Dropout\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.transforms import ToTensor,Compose\n","from torch.optim import SparseAdam,Adam,Adagrad,SGD\n","\n","# colab\n","# from google.colab import drive  "]},{"cell_type":"markdown","metadata":{"id":"yNInRwuOwRsQ"},"source":["**Hide Warnings**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGl6YxtjwRsR"},"outputs":[],"source":["warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"lqWShK-EwRsR"},"source":["**Disable Autoscrolling**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U63LxRbUwRsR","outputId":"c6cffba1-412a-4842-8a6f-e9a8f39f47e5"},"outputs":[],"source":["%%javascript\n","IPython.OutputArea.prototype._should_scroll = function(lines) {\n","    return false;\n","};"]},{"cell_type":"markdown","metadata":{"id":"AJxZJ6ijwRsR"},"source":["<br><br><br>\n","<br><br><br>\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"l6lCdQbzwRsR"},"source":["## Question 1: Dataset Preparation (Ingestion)\n","\n","---\n","\n","\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"9pOmzAspwRsR"},"source":["This implementation contains one file for training and two files for testing:   \n","- ml-1m.train.rating   \n","- ml-1m.test.rating  \n","- ml-1m.test.negative   \n","\n","<br>\n","(feel free to use visual explanations)\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"4WNea9kswRsR"},"source":["a. **Explain** the role and structure of each file and how it was created from the original <a href='https://github.com/hexiangnan/neural_collaborative_filtering/tree/master/Data'>MovieLens 1M rating dataset</a>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_diNyPQCwRsS"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9C6h5NW8wRsS"},"source":["b. **Explain** how the training dataset is created."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- We iterate through each line of the train file.\n","- We find the number of items and users.\n","- We create a sparse matrix with NXM (users X items)\n","- We go through the file again and for each user item pair in the file line we check if there has been any positive rating.\n","- If there has been a positive rating we add a binary indicator of implicit feedback (1)."]},{"cell_type":"markdown","metadata":{"id":"AknICV35wRsS"},"source":["c. **Explain** how the test dataset is created."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Test dataset has been created in a similar manner to the train dataset, except for the negative samples which have been created as vector.\n","Each line contains 99 negative samples(the id of the user matches the index of the line)."]},{"cell_type":"markdown","metadata":{"id":"BYdz5IwnwRsS"},"source":["#### Data Preperations:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HEvP5dKkwRsS"},"outputs":[],"source":["# urls\n","train_url = \"https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/Data/ml-1m.train.rating?raw=true\"\n","test_url = \"https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/Data/ml-1m.test.rating?raw=true\"\n","test_neg_url = \"https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/Data/ml-1m.test.negative?raw=true\"\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import requests\n","\n","def get_num_users_and_items(data_lines):\n","    num_users = 0\n","    num_items = 0\n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        user_id = int(line_values[0])\n","        item_id = int(line_values[1])\n","        num_users = max(num_users, user_id)\n","        num_items = max(num_items, item_id)\n","\n","    return num_users, num_items\n","\n","def generate_matrix(data_lines, num_users, num_items):\n","    matrix = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        user_id = int(line_values[0])\n","        item_id = int(line_values[1])\n","        rating = float(line_values[2])\n","        if rating > 0:\n","            matrix[user_id, item_id] = 1.0\n","    \n","    return matrix\n","    \n","\n","def load_data_as_matrix(url):\n","    # download data with requests\n","    response = requests.get(url)\n","    # Read as a text file\n","    data = response.text\n","    # Split by lines\n","    data_lines = data.splitlines()\n","    # Get number of users and items\n","    num_users, num_items = get_num_users_and_items(data_lines)\\\n","    # Construct matrix\n","    mat = generate_matrix(data_lines, num_users, num_items)\n","    \n","    return mat\n","\n","def load_negatives_vector(url):\n","    # download data with requests\n","    response = requests.get(url)\n","    # Read as a text file\n","    data = response.text\n","    # Split by lines\n","    data_lines = data.splitlines()\n","    # Construct vector\n","    vector = []\n","    for line in data_lines:\n","        line_values = line.split(\"\\t\")\n","        vector.append([int(x) for x in line_values[1:]])\n","    \n","    return vector\n","     \n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VpxacomIwRsS"},"outputs":[],"source":["train_matrix = load_data_as_matrix(train_url)\n","test_matrix = load_data_as_matrix(test_url)\n","test_neg_vector = load_negatives_vector(test_neg_url)\n","\n","num_users, num_items = train_matrix.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_76sxTaFwRsS","outputId":"35463ebf-4362-43b9-fda3-082295f80f06"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5OgOFP1BwRsS"},"source":["<br><br><br>\n","<br><br><br>\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"BZohC5TfwRsS"},"source":["## Question 2: Neural Collaborative Filtering \n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"9Ux4hWQPwRsS"},"source":["## a. Build the following four models using the neural collaborative filtering approach: \n","- Matrix Factorization (MF)\n","- Multi layer perceptron (MLP)\n","- Generalized Matrix Factorization (GMF) \n","- NeuroMatrixFactorization (NMF)\n","\n","**For each model, use the best hyper-parameters suggested in the neuMF paper.**"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Enum of the different models\n","class ModelType:\n","    MF = 0\n","    MLP = 1\n","    GMF = 2\n","    NeuMF = 3"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Define the model\n","class NCF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_size=10, mlp_layers_sizes=[10, 10], dropout=0.1, model_type=ModelType.NeuMF):\n","        super(NCF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.emb_size = embedding_size\n","        self.mlp_layers_sizes = mlp_layers_sizes\n","        self.dropout = dropout\n","\n","        if model_type != ModelType.MLP:\n","            self.emb_user_MF = nn.Embedding(num_embeddings=num_users, embedding_dim=1)\n","            self.emb_item_MF = nn.Embedding(num_embeddings=num_items, embedding_dim=1)\n","            self.gmf_layer = self._create_GMF_layer()\n","\n","        if model_type != ModelType.MF and model_type != ModelType.GMF:\n","            self.MLP_layer = self._create_MLP_layer()\n","            self.emb_user_MLP = nn.Embedding(num_embeddings=num_users, embedding_dim=self.emb_size)\n","            self.emb_item_MLP = nn.Embedding(num_embeddings=num_items, embedding_dim=self.emb_size)\n","        \n","        self._init_weights()\n","        self.neu_mf = self._create_NeuMF_layer()\n","\n","    def _init_weights(self):\n","        if self.emb_user_MLP is not None and self.emb_item_MLP is not None:\n","            nn.init.normal_(self.emb_user_MLP.weight, std=0.01)\n","            nn.init.normal_(self.emb_item_MLP.weight, std=0.01)\n","        if self.emb_user_MF is not None and self.emb_item_MF is not None:\n","            nn.init.normal_(self.emb_user_MF.weight, std=0.01)\n","            nn.init.normal_(self.emb_item_MF.weight, std=0.01)\n","    \n","    def _create_MLP_layer(self):\n","        layers = []\n","        num_layer = len(self.mlp_layers_sizes)\n","        for i in range(num_layer):\n","            input_size = self.emb_size if i == 0 else self.mlp_layers_sizes[i-1]\n","            layers.append(nn.Linear(input_size, self.mlp_layers_sizes[i]))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.Dropout(p=self.dropout))\n","        return nn.Sequential(*layers)\n","    \n","    def _create_NEU_MF_layer(self):\n","        return nn.Sequential(\n","            nn.Linear(in_features=2, out_features=1),\n","            nn.Sigmoid()\n","        )\n","    \n","    def _create_GMF_layer(self):\n","        return nn.Sequential(\n","            nn.Linear(in_features=1, out_features=1),\n","            nn.ReLU()\n","        )\n","    \n","    def forward(self, users, items):\n","        # GMF + MF\n","        if self.model_type != ModelType.MLP:\n","            emb_user_MF = self.emb_user_MF(users)\n","            emb_item_MF = self.emb_item_MF(items)\n","            # IF it's only MF return UxI\n","            if self.model_type == ModelType.MF:\n","                return emb_user_MF @ emb_item_MF.T\n","            \n","            gmf_vector = emb_user_MF * emb_item_MF\n","            gmf_vector = self.gmf_layer(gmf_vector)\n","        \n","        # MLP\n","        if self.model_type != ModelType.GMF and self.model_type != ModelType.MF:\n","            emb_user_MLP = self.emb_user_MLP(users)\n","            emb_item_MLP = self.emb_item_MLP(items)\n","            mlp_vector = torch.cat([emb_user_MLP, emb_item_MLP], dim=-1)\n","            mlp_vector = self.MLP_layer(mlp_vector)\n","\n","        if self.model_type == ModelType.NeuMF:\n","            vector = torch.cat([mlp_vector, gmf_vector], dim=-1)\n","        elif self.model_type == ModelType.GMF:\n","            vector = gmf_vector\n","        else:\n","            vector = mlp_vector\n","        \n","        # NeuMF\n","        vector = self.neu_mf(vector)\n","        return vector "]},{"cell_type":"markdown","metadata":{"id":"5Duot--dwRsS"},"source":["<br><br><br><br>\n","#### Matrix Factorization (MF)  \n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jm5lWMRcwRsS"},"outputs":[],"source":["model_MF = NCF(num_users, num_items, 1, [], 0, ModelType.MF)"]},{"cell_type":"markdown","metadata":{"id":"vitvozFFEBzq"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgch4APIECF4"},"outputs":[],"source":["# display/print the model architecture\n","print(model_MF)"]},{"cell_type":"markdown","metadata":{"id":"Dh-zg0azwRsS"},"source":["<br><br><br><br><br><br>\n","#### Multi Layer Perceptron (MLP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Wp-qKa9wRsS"},"outputs":[],"source":["model_MLP = NCF(num_users, num_items, 1, [10, 10], 0.1, ModelType.MLP)"]},{"cell_type":"markdown","metadata":{"id":"JQQhkHQ8EWkM"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rucXJW0EWpa"},"outputs":[],"source":["# display/print the model architecture\n","print(model_MLP)"]},{"cell_type":"markdown","metadata":{"id":"D_CI9v2ewRsS"},"source":["<br><br><br><br><br><br>\n","####Generalized Matrix Factorization (GMF)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bD0w66AAwRsT"},"outputs":[],"source":["model_GMF = NCF(num_users, num_items, 1, [], 0, ModelType.GMF)"]},{"cell_type":"markdown","metadata":{"id":"4eSt1BbiEZqa"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clyfpE6BEZxR"},"outputs":[],"source":["# display/print the model architecture\n","print(model_GMF)"]},{"cell_type":"markdown","metadata":{"id":"M6BRGoaIwRsT"},"source":["<br><br><br><br><br><br>\n","#### NeuroMatrixFactorization (NMF)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Dl8tHwIwRsT"},"outputs":[],"source":["model_NMF = NCF(num_users, num_items, 1, [10, 10], 0.1, ModelType.NeuMF)"]},{"cell_type":"markdown","metadata":{"id":"0O-ttN8PEeNC"},"source":["Model's architecture:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bypupfsrEeSE"},"outputs":[],"source":["# display/print the model architecture\n","print(model_NMF)"]},{"cell_type":"markdown","metadata":{"id":"BxrAtkliwRsT"},"source":["<br><br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"xThyFGsHwRsT"},"source":["## b. Train and evaluate the recommendations accuracy of the models: \n","- MF\n","- GMF\n","- MLP\n","- NMF\n","\n","Compare the `LogLoss` and recommendations accuracy using `NDCG` and `MRR` metrics with cutoff values of 5 and 10.   \n","Discuss the comparison. "]},{"cell_type":"markdown","metadata":{"id":"eOKodzLSwRsT"},"source":["**Metrics:**\n","- HitRatio\n","- nDCG\n","- MRR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOHGH9acwRsT"},"outputs":[],"source":["# Use your own metrics implementation OR use external packages for the metrics.\n","# If you are using external packages make sure they work properly. \n","# A lot of the packages available does not work as you would expect."]},{"cell_type":"markdown","metadata":{"id":"PW2FyWqVwRsT"},"source":["**Evaluation:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KX6JtH5zwRsT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwPhLbr9wRsT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Ufqag0i5wRsT"},"source":["**HyperParams:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gf-Ku4H2wRsT"},"outputs":[],"source":["# the choosen hyperparams will effect your models & your grade "]},{"cell_type":"markdown","metadata":{"id":"Bb3_Vll6wRsT"},"source":["<br><br>\n","Create train data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_rwb27KwRsT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FvTC1ZP1wRsT"},"source":["<br><br>\n","train & eval:\n","- Create a training function \n","- Evaluate the models trained and save the results accordingly "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwH51NuBwRsT"},"outputs":[],"source":["# feel free to change the function signature\n","def model_train(model):\n","    pass\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mtJaEmEkwRsT"},"source":["<br><br><br><br>\n","<br><br><br><br>\n","<br><br><br><br>\n","All Results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_MmU9RQTwRsT","outputId":"628a9795-209a-4d81-a1ff-3ff6ffe47fde"},"outputs":[],"source":["# df_results\n","# each hyperparam will add a column to the dataframe\n","# this is an example for a df that would allow you to create plots easily"]},{"cell_type":"markdown","metadata":{"id":"lKiZrguTwRsT"},"source":["<br><br><br><br>\n","**Train & Validation Loss:**\n","\n","Make sure you did not overfit.  \n","In case you did, fix that by adding early-stopping, regularization, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZABlCT8wRsT"},"outputs":[],"source":["# plot\n","# make sure that you did not overfit!"]},{"cell_type":"markdown","metadata":{"id":"PlRPIAEwwRsU"},"source":["**Training Time:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMnMj7tkwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"GAMKjPbewRsU"},"source":["**Metric Evaluation:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRV3bz6_wRsU","outputId":"0e0a8f34-b8cf-48f9-90ae-7cd8f0476099"},"outputs":[],"source":["# plot suggestion/example - you may create your own plot (you should achieve higher results)"]},{"cell_type":"markdown","metadata":{"id":"UIZFWr8fwRsU"},"source":["<br><br><br><br>\n","<br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"5U2L74YEwRsU"},"source":["**c. How do the values of MRR and NDCG differ between your current model and the results you got in the previous exercises which implemented the explicit recommendation approach? What are the differences in preparing the dataset for evaluation?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OthggCt7wRsU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8cLPAs2BwRsU"},"source":["**d. How will you measure item similarity using the NeuMF model?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TksQLQ12wRsU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rrx0Eq3owRsU"},"source":["<br><br><br>\n","<br><br><br>\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"hYqvTGpawRsU"},"source":["## Question 3: Loss Function \n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"PMvPHonlwRsU"},"source":["#### a. One of the enhancements presented in the Neural Collaborative Filtering paper is the usage of probabilistic activation function (the sigmoid) and binary cross entropy loss function.    \n","\n","Select one of the models you implemented in question 2 and change the loss function to a `Mean Squared Error` and the activation function of the last layer to `RELU`.   \n","\n","Train the model and evaluate it in a similar way to what you did in question 2. \n","Compare the results and discuss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwoyVWp6wRsU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1YEAjeBdwRsU"},"source":["<br><br><br><br>\n","<br><br><br><br>\n","NMFs Results:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-W6ZWJrzwRsU"},"outputs":[],"source":["# example: df_results[df_results.model.str.startswith('NMF')]"]},{"cell_type":"markdown","metadata":{"id":"Y-4txgJ0wRsU"},"source":["<br><br><br>\n","<br><br><br>\n","\n","Train & Validation Loss:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sg45Gf3jwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"RMGjniP4wRsU"},"source":["<br><br><br>\n","Training Time:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIvazqFQwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"7w9hEWRhwRsU"},"source":["<br><br><br>\n","Metric Evaluation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2glYFiQwRsU"},"outputs":[],"source":["# plot"]},{"cell_type":"markdown","metadata":{"id":"FpDDwEKBwRsU"},"source":["<br><br><br><br>\n","**Conclusions:**\n","\n","    - In\n","    - Your\n","    - Own\n","    - Words\n","    "]},{"cell_type":"markdown","metadata":{"id":"hLwmMt95wRsV"},"source":["<br><br>\n","<br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"NE0-2ifMwRsV"},"source":["Good Luck :)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"}}},"nbformat":4,"nbformat_minor":0}
